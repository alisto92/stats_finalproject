---
title: 'Lab 3: Draft Report'
author: "Priscilla Burity, Oscar Linares, Alissa Stover"
date: "3/16/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# An Introduction

Research Question: Which policies are most promising in reducing the crime rate: economic (e.g., increasing wages) or classic criminal deterrants (e.g., increasing arrests) (or both together)? What is the influence of contextual factors (e.g., characteristics of a population) on these relationships? 

We will be modeling our dependent variable (crime rate, `crmrte`), using the following variables as candidate independent variables. We focus on variables that represent different economic and criminal justice policies, which can be influenced via various policy decisions taken by our candidate. 

We also include a range of contextual variables that help to control for other variables that could influence the efficacy of these policies. We aim to produce a model robust to these, that can help guide decisions at the state level and help reduce crime across these counties. 

## Actionable Variables

### Economic Policy

These variables describe things we could affect with economic policy. For example, if increased wages are found to be related significantly to crime, our candiate could consider strategies such as raising the minimum wage in an attempt to lower crime. These variables speak to wages for different sectors.

- `wcon`:	weekly wage, construction
- `wtuc`:	wkly wge, trns, util, commun
- `wtrd`:	wkly wge, whlesle, retail trade
- `wfir`:	wkly wge, fin, ins, real est
- `wser`:	wkly wge, service industry
- `wmfg`:	wkly wge, manufacturing
- `wfed`:	wkly wge, fed employees
- `wsta`:	wkly wge, state employees
- `wloc`:	wkly wge, local gov emps

The following variable represents tax policy, but it could also reflect overall wealth in a county. Holding tax rate constant, counties with more wealth will have more tax revenue and vice versa. Holding wealth constant, changing the tax rate would change the tax revenue per capita. A better measure of tax policy would be the actual tax rates in each county. Since the impact of policy on this variable is confounded by the overal wealth in a county, the causal relationship would be difficult to interpret. Thus, we will include this variable later in our modeling process as a contextual variable rather than as a main policy variable. 

- `taxpc`:	tax revenue per capita 

### Criminal Justice Policy

These variables describe things we could affect with policy around the criminal justice system. Many of these policies are used as deterrants of crime, with the assumption being that people inherently understand that these punishments exist and as such will be less likely to commit crimes (in order to avoid the punishment). 

Please note that the 3 "efficiency" variables below were initially called "probabilities". However, this is a misnomer as these variables are truly ratios (e.g., the ratio of arrests to crimes reported). We interpret these as measures of the efficiency of the criminal justice system in a county. Ideally, we would want to strike a balance between arresting enough people to prevent criminal activity, and not arresting so many as to abuse the populace.

*Efficiency of arrest* 

If we find that the efficiency of arrest is associated with crime rate (with higher punishment associted with higher crime), we may consider stricter policies around police practice that encourage more arrests. 

- `prbarr`: number of convictions for every crime reported

*Efficiency of conviction* 

If we find that the efficiency of conviction is associated with crime rate (with higher conviction rates associted with higher crime), we may consider stricter policies around court practices so that more arrests lead to convictions.

- `prbconv`: number of convictions for every arrest made

*Efficiency of sentencing* 

If we find that the efficiency of sentencing is associated with crime rate (with higher sentencing rates associted with higher crime), we may consider stricter policies around judicial practices so that more convictions lead to sentencing.

- `prbpris`: number of prison sentences for every conviction

We can combine the three above variables into one variable (`eff_cj`) that is a measure of the efficiency of the criminal justice process in each county: 

eff_cj <- `prbarr` * `prbconv` * `prbpris` 

*Severity of punishment* 

If we find that the severity of punishment is associated with crime rate (with greater severity associted with lower crime), we may consider stricter policies around judicial practices so that people receive more severe punishments if sentenced with a crime.

- `avgsen`:	avg. sentence, days 

*Number of policy officers per capita*

The number of police police officers per capita (measured by the variable
`polpc`) is a very intuitive example of crime deterrant, as it in theory
increases the likelyhood of being caught in the act of the crime and is
also a measure of the capacity of the State to enforce the law.

As our job is to advise a policy maker, it’s very important that we feel
safe about the causal interpretation of our findings. After all, the
idea is to propose policies with a significant impact on people’s
lives. For statistical rigor, we also need to make sure that causal effects go one way (that we have reason to believe that our independent variables have causal effects on our dependent variables, and not the reverse). A cross section of counties does not allow us to link
police presence to crime rates with a causal interpretation. This is
because one can expect the counties with larger crime rates in a year *t* would consider necessary to have a larger number of police officers in on the streets in year *t*, which can imply a positive correlation between presence of police and crime rates with no causal meaning. An increase in crime could result in an increase in the number of police officers *and* an increase in police officers could result in a decrease in crime. 

Thus, in this exercise we opted not to include police per capita as a
candidate policy variable or covariate. 

## Contextual Variables

Since we are aiming to produce policy recommendations, the following variables may be helpful in our analysis but are not necessarily actionable. They describe county characteristics that provide important information about which policies could work across different county contexts.

### Types of Crime 

- `mix`:	offense mix: face-to-face/other

### Demographics 

The following variables describe different aspects of the people that reside in these counties.

*Urban/rural dwellers* 

- `density`:	people per sq. mile 

*Minority status*

- `pctmin80`:	perc. minority, 1980 

*Gender & Age* (young males are more likely to enter the criminal justice system)

- `pctymle`:	percent young male 


### Geography

The following variables identify which region of the state each county is in, with the assumption that counties cluster geographically in terms of their culture and other characteristics we are not explicitly measuring.  

- `west`:	=1 if in western N.C.
- `central`:	=1 if in central N.C.
- `urban`:	=1 if in SMSA



Your introduction should present a research question and explain the concept that you're attempting to measure and how it will be operationalized. This section should pave the way for the body of the report, preparing the reader to understand why the models are constructed the way that they are. It is not enough to simply say "We are looking for determinants of crime." Your introduction must do work for you, focusing the reader on a specific measurement goal, making them care about it, and propeling the narrative forward. This is also good time to put your work into context, discuss cross-cutting issues, and assess the overall appropriateness of the data.

# A Model Building Process

## Data Cleaning

### Environment

#### Import Libraries 

```{r, message=FALSE}
library(tidyverse) # for data import, manipulation, viz
library(corrplot) # for correlation matix
library(stargazer) # visualize model fit
library(skimr) # generate summary statistics
library(car) # statistics 
library(lmtest) # linear modeling
```

#### Import Data

We are importing a CSV file into R. The argument `n_max` is set to the number of rows of data we are provided (91). 

```{r, message=FALSE}
data <- read_csv("crime_v2.csv", n_max = 91)
```

#### Exploratory Data Analysis: File-level data checks

Let's take a visual check of our data. We have 91 observations and 25 columns. This is what we expected based on our original csv file. 

```{r}
head(data)
dim(data)
```

All 25 columns are numeric. 

```{r}
str(data)
```

##### The Where & When of these data 

It looks like we usually have 1 observation for each of the counties -- except for county #193. We do not have any missing values in the `county` column. 

```{r}
length(unique(data$county))
table(data$county, useNA = "always")
```

Having 2 observations for county #193 is surprising. We can see that these rows appear to be exact duplicates. 

```{r}
data %>% filter(county == 193)
```

We can see that `year` appears to be constant (which is expected). We also don't have any missing values for this column. We can drop this column in future data processing steps. 

```{r}
table(data$year, useNA = "always")
```

Based on the explorations above, we create a new dataframe that has dropped our superfluous column (`year`) and the superfluous row (duplicate of county 193). 

```{r}
data2 <- data %>% distinct() %>% select(-year)
```

Now we have 90 rows and 24 variables, with one observation per county.

```{r}
length(unique(data2$county)) == length(data2$county) # check 1 obs per county
str(data2) # examine dataframe 
```

##### Missingness & Data Validity

We can see from the following table that we don't have any missing data, which is good! 

However, we can see by the max for each column (p100), that we appear to have a few values that are extremely high. 

For example, one county has an average sentence length (`avgsen`) of 20.7 years.

```{r}
data2_skim <- skim(data2)
data2_skim
```

##### Preserve Clean Data

Save our clean dataset

```{r}
save(data2, file = "data_clean.rda")
```

## Exploratory Data Analysis: Variable-level data checks

### Dependent Variable: Crime Rate

Barring extreme issues with `crmrte` (crime rate), we would recommend focusing on this as our target variable. Although arrest, conviction, and sentencing rates are important, what matters most to people is the level of crime around them every day. The justice system is used to enact justice and to deter crimes. Justice is important, and making sure that those that commit crimes are punished is one way of going about achieving a just society. However, justice is a moral issue and can be seen from many different lenses. On the other hand, most people would agree that not having a lot of crime around them - and the sense of safety that comes from it - is very important. Thus we will use these quantitative methods to evaluate overall crime, rather than try to monitor the fuzzier concept of "justice". While doing so, we will also make sure to evaluate the arrest/conviction/sentencing variables to see whether they are related to different levels of crime - whether they could be having any effect as crime deterrents would be important for policymakers to know. 

We don't have enough data points to make a strong claim about the distribution of `crmtre`, however it appears to be approximately normal (with a slight skew which could be an artifact of not having any values below 0). 

```{r}
ggplot(data2, aes(crmrte)) +
  geom_histogram(bins = 35) +
  theme_minimal() +
  xlab("Crime Rate") +
  ylab("Count") +
  ggtitle("Histogram of Candidate Dependent Variable: Crime Rate")
```

From our boxplot we can see that most values are between 0.02 and 0.04, however we do have a range of values including some that are close to 0.10. We don't appear to have any spurious values. 

```{r}
ggplot(data2, aes(y = crmrte)) +
  geom_boxplot() +
  theme_minimal() +
  ylab("Crime Rate") +
  ggtitle("Boxplot of Candidate Dependent Variable: Crime Rate")
```

Our preferred variable (`crmrte`) appears to be of sufficient quality for our analyses and thus we will use it as our dependent variable. 

### Independent Variables 

#### Actionable Variables

##### Economic Policy

One of the things one can affect with economic policy are wages. In North Carolina in 1987, the minimum hourly wage was \$3.35. Given that most people work 40 hours per week, the minimum hourly wage would be \$134 per week.

In the plots below, we plot weekly wage by sector with a red vertical line indicating this minimum weekly wage. 

We can see that `wrtd` (Wholesale/Retail/Trade) wages are closest to the minimum wage - which would suggest that wages in this sector would be most influenced by changes to the minimum wage. The next sector that would be influenced is `wser` (Service Industry). However, `wser` appears to have an outlier (someone is earning over $2000 weekly, which is well above the weekly wages for any other sector as well as `wser`). Additionally, `wser` may be a messier measure of income because many in that industry work in restaurants and make additional income through tips - however, some people may not report this income. Meanwhile, those working in Wholesale/Retail/Trade may be more likely to work in places like malls, which would not provide additional income as tips and would be a better measure of low-income wages. 



```{r}
# format data for subplots 
data_ep_long <- data2 %>% 
  select(wcon, wtuc, wtrd, wfir, wser, wmfg, wfed, wsta, wloc) %>%
  gather(key = var, value = value)

data_ep_long$var <- factor(data_ep_long$var,
                           levels = c("wcon", "wtuc", "wtrd", "wfir",
                                      "wser", "wmfg", "wfed", "wsta", "wloc"),
                           labels = c("Construction", 
                                      "Utilities/Transport", 
                                      "Wholesale/Retail/Trade",
                                      "Finance/Insurance/Real Estate",
                                      "Service Industry",
                                      "Manufacturing",
                                      "Federal Emp",
                                      "State Emp",
                                      "Local/State Emp"))
```

```{r}
# plot hist subplots
ggplot(data_ep_long, aes(value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~var) +
  theme_minimal() +
  ggtitle("Histogram of Economic Policy Variables: Wages by Sector") +
  geom_vline(aes(xintercept = 134, color = "red")) +
  theme(legend.position = "none")

```

```{r}
# plot boxplot subplots 
ggplot(data_ep_long, aes(x = var, y = value)) +
  geom_boxplot() +
  coord_flip() + 
  theme_minimal() +
  xlab("") + 
  ylab("Weekly Wage") +
  ggtitle("Boxplot of Economic Policy Variables: Wages by Sector")+
  geom_hline(aes(yintercept = 134, color = "red")) +
  theme(legend.position = "none")

```

##### Criminal Justice Policy

```{r}
# format data for subplots 
data_cj_long <- data2 %>% select(prbarr, prbconv, prbpris) %>%
  gather(key = var, value = value)

# convert to factor for subplot labelling 
data_cj_long$var <- factor(data_cj_long$var, labels = c("Prob Arrest", "Prob Convicted", "Prob Sentenced"))
```

We can see that we have different distributions for these, but that they all could be considered somewhat close to normal, with different levels of skew. efficiency of conviction appears to be closest to a normal distribution. 

```{r}
# plot hist subplots
ggplot(data_cj_long, aes(value)) +
  geom_histogram(bins = 50) +
  facet_grid(~var) +
  theme_minimal() +
  ggtitle("Histogram of Criminal Justice Policy Variables: efficiency Variables")
```

From our boxplots we can see that we have some extreme values for efficiency of conviction, with multiple counties having a rate above 1. As these are not true probabilities, but rather ratios, these values are not necessarily spurious. However, they imply that for each arrest, there could be multiple convictions - one county has over 2 convictions for every arrest. Only 1 county has more than 1 arrest per crime committed - most counties in fact have a much lower rate (and only put someone under arrest for about one-quarter of crimes). When it comes to setencing, counties show a much lower spread and cluster around 1 conviction resulting in a prison sentence for every 1 conviction that does not. 

```{r}
# plot boxplot subplots 
ggplot(data_cj_long, aes(x = var, y = value)) +
  geom_boxplot() +
  theme_minimal() +
  xlab("") +
  ylab("Ratio") + 
  ggtitle("Boxplot of Criminal Justice Policy Variables: efficiency Variables")
```


```{r}
# plot hist
ggplot(data2, aes(avgsen)) +
  geom_histogram(bins = 25) +
  theme_minimal() +
  ggtitle("Histogram of Criminal Justice Policy Variables: Severity") +
  xlab("Average Sentence Severity")
```

```{r}
# plot hist
ggplot(data2, aes(y = avgsen)) +
  geom_boxplot() +
  theme_minimal() +
  ggtitle("Boxplot of Criminal Justice Policy Variables: Severity") +
  ylab("Average Sentence Severity")
```


#### Contextual Variables

### Types of Crime 

```{r}
# plot barplots
ggplot(data2, aes(mix)) +
  geom_histogram(bins = 35) +
  theme_minimal() +
  ggtitle("Histogram of Contextual Variables: Type of Crime") +
  xlab("offense mix: face-to-face/other")
```

##### Demographics 

*Urban/rural dwellers* 

```{r}
ggplot(data2, aes(density)) +
  geom_histogram(bins = 35) +
  theme_minimal() +
  xlab("Density") +
  ylab("Count") +
  ggtitle("Histogram of Contextual Variables: Population Density")
```


```{r}
ggplot(data2, aes(y = density)) +
  geom_boxplot() +
  theme_minimal() +
  ylab("Density") +
  ggtitle("Boxplot of Contextual Variables: Population Density")
```

*Minority status*

```{r}
ggplot(data2, aes(pctmin80)) +
  geom_histogram(bins = 40) +
  theme_minimal() +
  xlab("% Minority") +
  ylab("Count") +
  ggtitle("Histogram of Contextual Variables: Percentage Minority (1980)")
```


```{r}
ggplot(data2, aes(y = pctmin80)) +
  geom_boxplot() +
  theme_minimal() +
  ylab("% Minority") +
  ggtitle("Boxplot of Contextual Variables: Percentage Minority (1980)")
```

- `pctmin80`:	perc. minority, 1980 

*Gender & Age* (young males are more likely to enter the criminal justice system)

```{r}
ggplot(data2, aes(pctymle)) +
  geom_histogram(bins = 25) +
  theme_minimal() +
  xlab("% Young Males") +
  ylab("Count") +
  ggtitle("Histogram of Contextual Variables: Percentage Young Males")
```

```{r}
ggplot(data2, aes(y = pctymle)) +
  geom_boxplot() +
  theme_minimal() +
  ylab("% Young Males") +
  ggtitle("Boxplot of Contextual Variables: Percentage Young Males")
```

- `pctymle`:	percent young male 

*Wealth*

```{r}
ggplot(data2, aes(taxpc)) +
  geom_histogram(bins = 25) +
  theme_minimal() +
  xlab("Tax Revenue Per Capita") +
  ylab("Count") +
  ggtitle("Histogram of Contextual Variables: Tax Revenue Per Capita")
```

```{r}
ggplot(data2, aes(y = taxpc)) +
  geom_boxplot() +
  theme_minimal() +
  ylab("Tax Revenue Per Capita") +
  ggtitle("Boxplot of Contextual Variables: Tax Revenue Per Capita")
```

- `taxpc`:	tax revenue per capita 

##### Geography

```{r}
# format data for subplots 
data_geo_long <- data2 %>% select(west, central, urban) %>%
  gather(key = var, value = value)
```

```{r}
# plot barplots
ggplot(data_geo_long, aes(value)) +
  geom_bar() +
  facet_grid(~var) +
  theme_minimal() +
  ggtitle("Barplot of Contextual Variables: Geography")
```

- `west`:	=1 if in western N.C.
- `central`:	=1 if in central N.C.
- `urban`:	=1 if in SMSA


### Relationships between Variables

One would expect variables within each of the above groups to be correlated to each other. 

This is a correlation matrix of all of our variables, using Spearman's due to the fact that not all variables follow a normal distribution. 

```{r}
data_corr <- data2 %>% select(-county) 

matrix_corr <- round(cor(data_corr, method = "spearman"), 1)
```
```{r}
corrplot(matrix_corr, type = "lower", method = "ellipse", order = "alphabet")
```

## Results

You will next build a set of models to investigate your research question, documenting your decisions. Here are some things to keep in mind during your model building process:

What do you want to measure? Make sure you identify one, or a few, variables that will allow you to derive conclusions relevant to the political campaign, and include those variables in all model specifications.

What covariates help you correctly and accurately measure a causal effect? What covariates are problematic, either due to multicollinearity, or because they will absorb some of a causal effect you want to measure?

What transformations should you apply to each variable? This is very important because transformations can reveal linearities in the data, make our results relevant, or help us meet model assumptions.

Are your choices supported by EDA? You will likely start with some general EDA to detect anomalies (missing values, top-coded variables, etc.). From then on, your EDA should be interspersed with your model building. Use visual tools to guide your decisions. You can also leverage statistical tests to help assess whether variables, or groups of variables, are improving model fit.

At the same time, it is important to remember that you are not trying to create one perfect model. You will create several specifications, giving the reader a sense of how robust your results are (how sensitive to modeling choices), and to show that you're not just cherry-picking the specification that leads to the largest effects.

Guided by your background knowledge and your EDA, other specifications may make sense. You are trying to choose points that encircle the space of reasonable modeling choices, to give an overall understanding of how these choices impact results.

At a minimum, you should include the following three specifications:

**Model 1**

One model with only the key variables you want to measure (possibly transformed, as determined by your EDA), and no other covariates (or perhaps one or at more two covariates if they are so crucial that it would be unreasonable to omit them)

**Model 2**

One model that includes key explanatory variables and covariates that you believe advance your modeling goals without introducing too much multicollinearity or causing other issues. This model should strike a balance between accuracy and parsimony and reflect your best understanding of the relationships among key variables.

**Model 3**

One model that includes the previous covariates, and most, if not all, other covariates. A key purpose of this model is to demonstrate the robustness of your results to model specification. (However, you should still not include variables that are clearly unreasonable. For example, don't include outcome variables that will absorb some of the causal effect you are interested in measuring)

**An Assessment of the CLM Assumptions**

For one of your model specifications, we would like to see a complete assessment of all 6 classical linear model assumptions.

Use plots and other diagnostic tools to assess whether the assumptions appear to be violated, and follow best practices in responding to any violations you find.

Note that we only want to see this level of detail for one model specification. For the other specifications, you should keep the CLM assumptions in mind, but only discuss them in your report if you encounter any major surprises.

Note that you may need to change your model specifications in response to violations of the CLM.

**Zero Conditional Mean:** 

**Normality of Errors:**

**Heteroskadiscity:**

**Outliers:**

## Alissa

**Model 1** 

DV: 

*Crime*

log(`crmrte`)

* take the log because it makes intuitive sense -- represents increase in crime 

We originally used the following variables to predict crime: 

*Crime policy* 

(1) log(eff_cj) <- log(`prbarr` * `prbconv` * `prbpris`)

(2) log(`avg_sen`)

*Economic policy* 

Picked lower wages (based on boxplots)* -- would be more likely to affect w/ min wage policy 
econ policy --> 

(3) log(`wtrd`)

However, `eff_cf` was significant for the criminal justice policy variables, thus we removed `avg_sen`. Our economic policy variable,`wtrd`, was significantly related to crime but not in the expected direction - the higher the wages, the higher the crime. Since `wtrd` and `density` are positively correlated, we thought that perhaps wages in this sector would be higher in cities and this omitted variable could be influencing the direction of the relationship between `wtrd` and `crmrte`.  

```{r}
data2 <- data2 %>% mutate(eff_cj = prbarr*prbconv*prbpris)
```

Approximately 30% of the variation in crime rate can be explained by the efficiency of the criminal justice process. Increasing the ratio of criminal justice practices to crime (increasing arrests, convictions, and sentencing) could help decrease the crime rate. 

```{r}
mod1 <- lm(log(crmrte) ~ log(eff_cj), data = data2)
stargazer(mod1, type = "text")
```


*need to check assumptions* 

**Model 2**  -- omitted variable bias --> add in these contextual variables 

In our first model, we only explained 30% of the variation in crime. Additionally, only criminal justice policy appeared to be related. However, one can imagine that other factors (such as population density or demographic characteristics such as gender & race) would also be at play when it comes to crime. Thus, in our second model we included important covariates that could affect the relationship between criminal justice policy and crime. 

* `mix` -- type of crime 
  * More severe crime (face-to-face) could be related criminal justice policies -- direction is unclear 
  * Unclear what relationship would be overall crime 
* `density` -- more crime if more people 
  * More people = more opportunity for crime (especially face to face)
* `pctymle` -- riskier pop
  * More young males, more crime
* `pctmin80` -- minority status
  * More minorities, more reported crime, more severe criminal justice practice 
* `taxpc` -- wealth
  * More wealth, less reason for crime -- less crime 
  * More tax revenue, more money for government to fight crime both via criminal justice & economic system 
  
* `pctmin80` * `density` -- number of minorities per squared mile
  * More minorities, more reported crime, more severe criminal justice practice 
  

```{r}
mod2 <- lm(log(crmrte) ~ log(eff_cj) + log(density *pctmin80) , data = data2)
stargazer(mod2, type = "text")
```




# A Regression Table
You should display all of your model specifications in a regression table, using a package like stargazer to format your output. It should be easy for the reader to find the coefficients that represent key effects near the top of the regression table, and scan horizontally to see how they change from specification to specification. Make sure that you display the most appropriate standard errors in your table, along with significance stars.

In your text, comment on both statistical significance and practical significance. You may want to include statistical tests besides the standard t-tests for regression coefficients.

# A Discussion of Omitted Variables
Identify what you think are the 5-10 most important omitted variables that bias results you care about. For each variable, you should estimate what direction the bias is in. If you can argue whether the bias is large or small, that is even better. State whether you have any variables available that may proxy (even imperfectly) for the omitted variable. Pay particular attention to whether each omitted variable bias is towards zero or away from zero. You will use this information to judge whether the effects you find are likely to be real, or whether they might be entirely an artifact of omitted variable bias.

# A Conclusion
Make sure that you end your report with a discussion that relates your results to concerns of the political campaign.

Submission

Submit your lab via ISVC; please do not submit via email.

Submit 2 files:

A pdf file including the summary, the details of your analysis, and all the R codes used to produce the analysis. Please do not suppress the code in your pdf file.

The Rmd or ipynb source file used to produce the pdf file.

Each group only needs to submit one set of files.

Be sure to include the names of all team members in your report. Place the word 'draft' in the file names.

Please limit your submission to 8000 words, excluding code cells and R output.