---
title: 'Lab 3: Draft Report'
author: "Priscilla Burity, Oscar Linares, Alissa Stover"
date: "3/16/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# An Introduction

Research Question: Which policies are most promising in reducing the crime rate, economic (e.g., increasing wages) or classic criminal deterrants (e.g., increasing arrests), when holding other variables (e.g., characteristics of a population) constant? Do they work best independently or in conjunction? 

We will be modeling our dependent variable (crime rate), using the following variables as candidate independent variables. We focus on variables that represent different economic and criminal justice policies, which can be influenced via various policy decisions taken by our candidate. 

We also include a range of contextual variables that help to control for other variables that could influence the efficacy of these policies. We aim to produce a model robust to these, that can help guide decisions at the state level and help reduce crime across these counties. 

## Actionable Variables

### Economic Policy

These variables describe things we could affect with economic policy. For example, if increased wages are found to be related significantly to crime, our candiate could consider strategies such as raising the minimum wage in an attempt to lower crime. These variables speak to wages for different sectors.

- `wcon`:	weekly wage, construction
- `wtuc`:	wkly wge, trns, util, commun
- `wtrd`:	wkly wge, whlesle, retail trade
- `wfir`:	wkly wge, fin, ins, real est
- `wser`:	wkly wge, service industry
- `wmfg`:	wkly wge, manufacturing
- `wfed`:	wkly wge, fed employees
- `wsta`:	wkly wge, state employees
- `wloc`:	wkly wge, local gov emps

### Criminal Justice Policy

These variables describe things we could affect with policy around the criminal justice system. Many of these policies are used as deterrants of crime, with the assumption being that people inherently understand that these punishments exist and as such will be less likely to commit crimes (in order to avoid the punishment). 

*Probability of arrest* 

If we find that the probability of arrest is associated with crime rate (with higher punishment associted with higher crime), we may consider stricter policies around police practice that encourage more arrests. 

- `prbarr`: 'probability' of arrest

*Probability of conviction* 

If we find that the probability of conviction is associated with crime rate (with higher conviction rates associted with higher crime), we may consider stricter policies around court practices so that more arrests lead to convictions.

- `prbconv`: 'probability' of conviction

*Probability of sentencing* 

If we find that the probability of sentencing is associated with crime rate (with higher sentencing rates associted with higher crime), we may consider stricter policies around judicial practices so that more convictions lead to sentencing.

- `prbpris`: 'probability' of prison sentence 

*Severity of punishment* 

If we find that the severity of punishment is associated with crime rate (with greater severity associted with lower crime), we may consider stricter policies around judicial practices so that people receive more severe punishments if sentenced with a crime.

- `avgsen`:	avg. sentence, days 


## Contextual Variables

Since we are aiming to produce policy recommendations, the following variables may be helpful in our analysis but are not necessarily actionable. They describe county characteristics that provide important information about which policies could work across different county contexts.

### Types of Crime 

- `mix`:	offense mix: face-to-face/other

### Demographics 

The following variables describe different aspects of the people that reside in these counties.

*Urban/rural dwellers* 

- `density`:	people per sq. mile 

*Minority status*

- `pctmin80`:	perc. minority, 1980 

*Gender & Age* (young males are more likely to enter the criminal justice system)

- `pctymle`:	percent young male 

*Wealth*

- `taxpc`:	tax revenue per capita 

### Geography

The following variables identify which region of the state each county is in, with the assumption that counties cluster geographically in terms of their culture and other characteristics we are not explicitly measuring.  

- `west`:	=1 if in western N.C.
- `central`:	=1 if in central N.C.
- `urban`:	=1 if in SMSA



Your introduction should present a research question and explain the concept that you're attempting to measure and how it will be operationalized. This section should pave the way for the body of the report, preparing the reader to understand why the models are constructed the way that they are. It is not enough to simply say "We are looking for determinants of crime." Your introduction must do work for you, focusing the reader on a specific measurement goal, making them care about it, and propeling the narrative forward. This is also good time to put your work into context, discuss cross-cutting issues, and assess the overall appropriateness of the data.

# A Model Building Process

## Data Cleaning

### Environment

#### Import Libraries 

```{r, message=FALSE}
library(tidyverse) # for data import, manipulation, viz
library(corrplot) # for correlation matix
library(stargazer) # visualize model fit
```

#### Import Data

We are importing a CSV file into R. The argument `n_max` is set to the number of rows of data we are provided (91). 

```{r, message=FALSE}
data <- read_csv("crime_v2.csv", n_max = 91)
```

#### Exploratory Data Analysis: File-level data checks

Let's take a visual check of our data. We have 91 observations and 25 columns. This is what we expected based on our original csv file. 

```{r}
head(data)
dim(data)
```

All 25 columns are numeric. 

```{r}
str(data)
```

##### The Where & When of these data 

It looks like we usually have 1 observation for each of the counties -- except for county #193. We do not have any missing values in the `county` column. 

```{r}
length(unique(data$county))
table(data$county, useNA = "always")
```

Having 2 observations for county #193 is surprising. We can see that these rows appear to be exact duplicates. 

```{r}
data %>% filter(county == 193)
```

We can see that `year` appears to be constant (which is expected). We also don't have any missing values for this column. We can drop this column in future data processing steps. 

```{r}
table(data$year, useNA = "always")
```

Based on the explorations above, we create a new dataframe that has dropped our superfluous column (`year`) and the superfluous row (duplicate of county 193). 

```{r}
data2 <- data %>% distinct() %>% select(-year)
```

Now we have 90 rows and 24 variables, with one observation per county.

```{r}
length(unique(data2$county)) == length(data2$county) # check 1 obs per county
str(data2) # examine dataframe 
```

##### Missingness

We can see from the following that we don't have any missing data, which is good! 

```{r}
pct_miss <- data %>% map(~ mean(is.na(.)))
pct_miss
```

##### Preserve Clean Data

Save our clean dataset

```{r}
save(data2, file = "data_clean.rda")
```

## Exploratory Data Analysis: Variable-level data checks

### Dependent Variable: Crime Rate

Barring extreme issues with `crmrte` (crime rate), we would recommend focusing on this as our target variable. Although arrest, conviction, and sentencing rates are important, what matters most to people is the level of crime around them every day. The justice system is used to enact justice and to deter crimes. Justice is important, and making sure that those that commit crimes are punished is one way of going about achieving a just society. However, justice is a moral issue and can be seen from many different lenses. On the other hand, most people would agree that not having a lot of crime around them - and the sense of safety that comes from it - is very important. Thus we will use these quantitative methods to evaluate overall crime, rather than try to monitor the fuzzier concept of "justice". While doing so, we will also make sure to evaluate the arrest/conviction/sentencing variables to see whether they are related to different levels of crime - whether they could be having any effect as crime deterrents would be important for policymakers to know. 

We don't have enough data points to make a strong claim about the distribution of `crmtre`, however it appears to be approximately normal (with a slight skew which could be an artifact of not having any values below 0). 

```{r}
ggplot(data2, aes(crmrte)) +
  geom_histogram(bins = 35) +
  theme_minimal() +
  xlab("Crime Rate") +
  ylab("Count") +
  ggtitle("Histogram of Candidate Dependent Variable: Crime Rate")
```

From our boxplot we can see that most values are between 0.02 and 0.04, however we do have a range of values including some that are close to 0.10. We don't appear to have any spurious values. 

```{r}
ggplot(data2, aes(y = crmrte)) +
  geom_boxplot() +
  theme_minimal() +
  ylab("Crime Rate") +
  ggtitle("Boxplot of Candidate Dependent Variable: Crime Rate")
```

Our preferred variable (`crmrte`) appears to be of sufficient quality for our analyses and thus we will use it as our dependent variable. 

### Independent Variables 

#### Actionable Variables

##### Economic Policy

```{r}
# format data for subplots 
data_ep_long <- data2 %>% 
  select(wcon, wtuc, wtrd, wfir, wser, wmfg, wfed, wsta, wloc) %>%
  gather(key = var, value = value)

data_ep_long$var <- factor(data_ep_long$var,
                           levels = c("wcon", "wtuc", "wtrd", "wfir",
                                      "wser", "wmfg", "wfed", "wsta", "wloc"),
                           labels = c("Construction", 
                                      "Utilities/Transport", 
                                      "Wholesale/Retail/Trade",
                                      "Finance/Insurance/Real Estate",
                                      "Service Industry",
                                      "Manufacturing",
                                      "Federal Emp",
                                      "State Emp",
                                      "Local/State Emp"))
```

```{r}
# plot hist subplots
ggplot(data_ep_long, aes(value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~var) +
  theme_minimal() +
  ggtitle("Histogram of Economic Policy Variables: Wages by Sector")

```

```{r}
# plot boxplot subplots 
ggplot(data_ep_long, aes(x = var, y = value)) +
  geom_boxplot() +
  coord_flip() + 
  theme_minimal() +
  xlab("") + 
  ylab("Weekly Wage") +
  ggtitle("Boxplot of Economic Policy Variables: Wages by Sector")

```

##### Criminal Justice Policy

```{r}
# format data for subplots 
data_cj_long <- data2 %>% select(prbarr, prbconv, prbpris) %>%
  gather(key = var, value = value)

# convert to factor for subplot labelling 
data_cj_long$var <- factor(data_cj_long$var, labels = c("Prob Arrest", "Prob Convicted", "Prob Sentenced"))
```

We can see that we have different distributions for these, but that they all could be considered somewhat close to normal, with different levels of skew. Probability of conviction appears to be closest to a normal distribution. 

```{r}
# plot hist subplots
ggplot(data_cj_long, aes(value)) +
  geom_histogram(bins = 50) +
  facet_grid(~var) +
  theme_minimal() +
  ggtitle("Histogram of Criminal Justice Policy Variables: Probability Variables")
```

From our boxplots we can see that we have some extreme values for probability of conviction, with multiple counties having a rate above 1. As these are not true probabilities, but rather ratios, these values are not necessarily spurious. However, they imply that for each arrest, there could be multiple convictions - one county has over 2 convictions for every arrest. Only 1 county has more than 1 arrest per crime committed - most counties in fact have a much lower rate (and only put someone under arrest for about one-quarter of crimes). When it comes to setencing, counties show a much lower spread and cluster around 1 conviction resulting in a prison sentence for every 1 conviction that does not. 

```{r}
# plot boxplot subplots 
ggplot(data_cj_long, aes(x = var, y = value)) +
  geom_boxplot() +
  theme_minimal() +
  xlab("") +
  ylab("Ratio") + 
  ggtitle("Boxplot of Criminal Justice Policy Variables: Probability Variables")
```


```{r}
# plot hist
ggplot(data2, aes(avgsen)) +
  geom_histogram(bins = 25) +
  theme_minimal() +
  ggtitle("Histogram of Criminal Justice Policy Variables: Severity") +
  xlab("Average Sentence Severity")
```

```{r}
# plot hist
ggplot(data2, aes(y = avgsen)) +
  geom_boxplot() +
  theme_minimal() +
  ggtitle("Boxplot of Criminal Justice Policy Variables: Severity") +
  ylab("Average Sentence Severity")
```


#### Contextual Variables

### Types of Crime 

```{r}
# plot barplots
ggplot(data2, aes(mix)) +
  geom_histogram(bins = 35) +
  theme_minimal() +
  ggtitle("Histogram of Contextual Variables: Type of Crime") +
  xlab("offense mix: face-to-face/other")
```

##### Demographics 

*Urban/rural dwellers* 

```{r}
ggplot(data2, aes(density)) +
  geom_histogram(bins = 35) +
  theme_minimal() +
  xlab("Density") +
  ylab("Count") +
  ggtitle("Histogram of Contextual Variables: Population Density")
```


```{r}
ggplot(data2, aes(y = density)) +
  geom_boxplot() +
  theme_minimal() +
  ylab("Density") +
  ggtitle("Boxplot of Contextual Variables: Population Density")
```

*Minority status*

```{r}
ggplot(data2, aes(pctmin80)) +
  geom_histogram(bins = 40) +
  theme_minimal() +
  xlab("% Minority") +
  ylab("Count") +
  ggtitle("Histogram of Contextual Variables: Percentage Minority (1980)")
```


```{r}
ggplot(data2, aes(y = pctmin80)) +
  geom_boxplot() +
  theme_minimal() +
  ylab("% Minority") +
  ggtitle("Boxplot of Contextual Variables: Percentage Minority (1980)")
```

- `pctmin80`:	perc. minority, 1980 

*Gender & Age* (young males are more likely to enter the criminal justice system)

```{r}
ggplot(data2, aes(pctymle)) +
  geom_histogram(bins = 25) +
  theme_minimal() +
  xlab("% Young Males") +
  ylab("Count") +
  ggtitle("Histogram of Contextual Variables: Percentage Young Males")
```

```{r}
ggplot(data2, aes(y = pctymle)) +
  geom_boxplot() +
  theme_minimal() +
  ylab("% Young Males") +
  ggtitle("Boxplot of Contextual Variables: Percentage Young Males")
```

- `pctymle`:	percent young male 

*Wealth*

```{r}
ggplot(data2, aes(taxpc)) +
  geom_histogram(bins = 25) +
  theme_minimal() +
  xlab("Tax Revenue Per Capita") +
  ylab("Count") +
  ggtitle("Histogram of Contextual Variables: Tax Revenue Per Capita")
```

```{r}
ggplot(data2, aes(y = taxpc)) +
  geom_boxplot() +
  theme_minimal() +
  ylab("Tax Revenue Per Capita") +
  ggtitle("Boxplot of Contextual Variables: Tax Revenue Per Capita")
```

- `taxpc`:	tax revenue per capita 

##### Geography

```{r}
# format data for subplots 
data_geo_long <- data2 %>% select(west, central, urban) %>%
  gather(key = var, value = value)
```

```{r}
# plot barplots
ggplot(data_geo_long, aes(value)) +
  geom_bar() +
  facet_grid(~var) +
  theme_minimal() +
  ggtitle("Barplot of Contextual Variables: Geography")
```

- `west`:	=1 if in western N.C.
- `central`:	=1 if in central N.C.
- `urban`:	=1 if in SMSA


### Relationships between Variables

One would expect variables within each of the above groups to be correlated to each other. 

```{r}
data_corr <- data2 %>% select(-county) 

matrix_corr <- round(cor(data_corr), 1)
```
```{r}
corrplot(matrix_corr, type = "lower", method = "ellipse", order = "alphabet")
```

## Results

- Discuss transformations
- Discuss results

### Priscilla 

```{r}

av_wage <- log(rowMeans(data2[c('wcon', 'wtuc', 'wtrd', 'wfir', 'wser', 'wmfg', 'wfed', 'wsta', 'wloc')]))

m1 <- lm(log(crmrte) ~ prbarr, data = data2) #only probs arrest, 
#here there are obvious ommited vars, as the presence of police 
m2 <- lm(log(crmrte) ~ prbarr + prbpris + log(avgsen), data = data2) #adding other law enforcement vars
m3 <- lm(log(crmrte) ~ prbarr + prbpris + log(avgsen) + log(polpc) + mix, data = data2) #also controlling for # of police p/c and offence mix
m4 <- lm(log(crmrte) ~  prbarr + prbpris + log(avgsen) + log(polpc) + mix + log(density) + 
           pctymle + pctmin80, data = data2) #adding demographic vars
m5 <- lm(log(crmrte) ~  prbarr + prbpris + log(avgsen) + log(polpc) + mix + log(density) + 
           pctymle + pctmin80 + west + central + urban, data = data2) # adding geographic data
m6 <- lm(log(crmrte) ~  prbarr + prbpris + log(avgsen) + log(polpc) + mix + log(density) + 
           pctymle + pctmin80 + west + central + urban + av_wage + log(taxpc), data = data2) # adding economic data
#m4 <- glm(fast ~ hp + drat + am, family=binomial(link="logit"), data=mydata) stargazer(m1, m2, m3, m4, type="text",
                                                                                       #dep.var.labels=c("Miles/(US) gallon","Fast car (=1)"), covariate.labels=c("Gross horsepower","Rear axle ratio","Four foward gears",
stargazer(m1, m2, m3, m4, m5, m6, type="text",
          dep.var.labels=c("ln(Crimes committed per person)"), covariate.labels=c("'Probability' of arrest", "'Probability' of prison sentence",   "ln(Avg. sentence, days)", "ln(Police per capita)",  "Offense mix: face-to-face/other",   "ln(People per sq. mile)",  "Percent young male", "Perc. minority, 1980", "=1 if in western N.C.", "=1 if in central N.C.",  "=1 if in SMSA", "ln(Average sector wkly wage)", "ln(Tax revenue per capita)"), out="models.txt")

```

### Alissa

DV: `crmrte`

IV: 
crime policy --> "prbarr", "prbconv", "avgsen" 
* Picked lower wages (based on boxplots)* -- would be more likely to affect w/ min wage policy 
econ policy --> "wser", "wtrd" 
covariates -- 
"mix"
"density" -- more crime if more people 
`pctymle` -- riskier pop
pctmin80 -- minority status


```{r}
m1 <- lm(crmrte ~ prbarr , data = data2)
summary(m1)
m2 <- lm(crmrte ~ prbarr + prbconv, data = data2)
summary(m2)
m3 <- lm(crmrte ~ prbarr + prbconv + log(polpc), data = data2)
summary(m3)
m4 <- lm(crmrte ~ prbarr + prbconv + log(polpc) + wser, data = data2)
summary(m4)
m5 <- lm(crmrte ~ prbarr + prbconv + log(polpc) + wser + wtrd, data = data2)
summary(m5)
m6 <- lm(crmrte ~ prbarr + prbconv + log(polpc) + wser + wtrd + mix, data = data2)
summary(m6)
m7 <- lm(crmrte ~ prbarr + prbconv + log(polpc) + wser + wtrd + mix + density, data = data2)
summary(m7)
m8 <- lm(crmrte ~ prbarr + prbconv + log(polpc) + wser + wtrd + mix + density + pctymle, data = data2)
summary(m8)
m9 <- lm(crmrte ~ prbarr + prbconv + log(polpc) + wser + wtrd + mix + density + pctymle + pctmin80, data = data2)
summary(m9)

# model w/o 
m10 <- lm(log(crmrte) ~ prbarr + prbconv + avgsen + wser + wtrd + mix + density + pctymle + pctmin80, data = data2)
summary(m10)

```

```{r}
m1 <- lm(log(crmrte) ~ prbarr , data = data2)
m2 <- lm(log(crmrte) ~ prbarr + prbconv, data = data2)
m3 <- lm(log(crmrte) ~ prbarr + prbconv + avgsen, data = data2)
m4 <- lm(log(crmrte) ~ prbarr + prbconv + avgsen + wser, data = data2)
m5 <- lm(log(crmrte) ~ prbarr + prbconv + avgsen + wser + wtrd, data = data2)
m6 <- lm(log(crmrte) ~ prbarr + prbconv + avgsen + wser + wtrd + mix, data = data2)
m7 <- lm(log(crmrte) ~ prbarr + prbconv + avgsen + wser + wtrd + mix + density, data = data2)
m8 <- lm(log(crmrte) ~ prbarr + prbconv + avgsen + wser + wtrd + mix + density + pctymle, data = data2)
m9 <- lm(log(crmrte) ~ prbarr + prbconv + avgsen + wser + wtrd + mix + density + pctymle + pctmin80, data = data2)
summary(m9)



```





You will next build a set of models to investigate your research question, documenting your decisions. Here are some things to keep in mind during your model building process:

What do you want to measure? Make sure you identify one, or a few, variables that will allow you to derive conclusions relevant to the political campaign, and include those variables in all model specifications.

What covariates help you correctly and accurately measure a causal effect? What covariates are problematic, either due to multicollinearity, or because they will absorb some of a causal effect you want to measure?

What transformations should you apply to each variable? This is very important because transformations can reveal linearities in the data, make our results relevant, or help us meet model assumptions.

Are your choices supported by EDA? You will likely start with some general EDA to detect anomalies (missing values, top-coded variables, etc.). From then on, your EDA should be interspersed with your model building. Use visual tools to guide your decisions. You can also leverage statistical tests to help assess whether variables, or groups of variables, are improving model fit.

At the same time, it is important to remember that you are not trying to create one perfect model. You will create several specifications, giving the reader a sense of how robust your results are (how sensitive to modeling choices), and to show that you're not just cherry-picking the specification that leads to the largest effects.

At a minimum, you should include the following three specifications:

One model with only the key variables you want to measure (possibly transformed, as determined by your EDA), and no other covariates (or perhaps one or at more two covariates if they are so crucial that it would be unreasonable to omit them)

One model that includes key explanatory variables and covariates that you believe advance your modeling goals without introducing too much multicollinearity or causing other issues. This model should strike a balance between accuracy and parsimony and reflect your best understanding of the relationships among key variables.

One model that includes the previous covariates, and most, if not all, other covariates. A key purpose of this model is to demonstrate the robustness of your results to model specification. (However, you should still not include variables that are clearly unreasonable. For example, don't include outcome variables that will absorb some of the causal effect you are interested in measuring)

Guided by your background knowledge and your EDA, other specifications may make sense. You are trying to choose points that encircle the space of reasonable modeling choices, to give an overall understanding of how these choices impact results.

# An Assessment of the CLM Assumptions
For one of your model specifications, we would like to see a complete assessment of all 6 classical linear model assumptions.

Use plots and other diagnostic tools to assess whether the assumptions appear to be violated, and follow best practices in responding to any violations you find.

Note that we only want to see this level of detail for one model specification. For the other specifications, you should keep the CLM assumptions in mind, but only discuss them in your report if you encounter any major surprises.

Note that you may need to change your model specifications in response to violations of the CLM.

# A Regression Table
You should display all of your model specifications in a regression table, using a package like stargazer to format your output. It should be easy for the reader to find the coefficients that represent key effects near the top of the regression table, and scan horizontally to see how they change from specification to specification. Make sure that you display the most appropriate standard errors in your table, along with significance stars.

In your text, comment on both statistical significance and practical significance. You may want to include statistical tests besides the standard t-tests for regression coefficients.

# A Discussion of Omitted Variables
Identify what you think are the 5-10 most important omitted variables that bias results you care about. For each variable, you should estimate what direction the bias is in. If you can argue whether the bias is large or small, that is even better. State whether you have any variables available that may proxy (even imperfectly) for the omitted variable. Pay particular attention to whether each omitted variable bias is towards zero or away from zero. You will use this information to judge whether the effects you find are likely to be real, or whether they might be entirely an artifact of omitted variable bias.

# A Conclusion
Make sure that you end your report with a discussion that relates your results to concerns of the political campaign.

Submission

Submit your lab via ISVC; please do not submit via email.

Submit 2 files:

A pdf file including the summary, the details of your analysis, and all the R codes used to produce the analysis. Please do not suppress the code in your pdf file.

The Rmd or ipynb source file used to produce the pdf file.

Each group only needs to submit one set of files.

Be sure to include the names of all team members in your report. Place the word 'draft' in the file names.

Please limit your submission to 8000 words, excluding code cells and R output.